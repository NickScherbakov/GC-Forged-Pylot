# 针对 VSCode 插件 (Cline、Roo 与 Continue) 的定制版 llama.cpp 服务器开发技术规格说明书

## 目标

开发并配置一个基于 llama.cpp 项目的本地 API 服务器。该服务器将与 VSCode 插件（Cline、Roo 与 Continue）集成，为代码编辑器内提供离线的智能代码辅助功能，确保所有数据均不传输至外部服务。

## 主要组成部分

1. **基础 llama.cpp 服务器**  
   针对 Intel i9-11900KF 处理器和 AMD RX 580 显卡进行了性能优化。

2. **增强型代理层**  
   - 支持函数调用（为外部工具提供功能支持）；  
   - 提供结构化的命令格式化输出；  
   - 实现代码库的语义搜索；  
   - 与主流搜索引擎集成；  
   - 对大规模项目进行上下文管理。

## 技术栈

- **服务端:** 使用 Node.js 与 Express 构建代理层。  
- **向量数据库:** 采用 FAISS 实现高速相似度搜索。  
- **代码处理:** 通过 Tree-sitter 完成代码的语义解析。  
- **模型管理:** 基于 llama.cpp，并结合 AVX-512 优化处理。

## 插件集成要求

- **针对 Continue 插件:**  
  实现结构化思维支持、上下文元素专用处理，以及与 VSCode 兼容的命令执行机制。  

- **针对 Roo 插件:**  
  提供函数调用 API、集成搜索引擎，以及高效的代码库管理工具。

## 实施步骤

1. **准备并构建 llama.cpp:**
   ```bash
   git clone https://github.com/ggml-org/llama.cpp
   cd llama.cpp
   make server
   ```

2. **下载并转换模型:**
   ```bash
   mkdir -p models
   python3 -m pip install huggingface_hub
   python3 -m huggingface_hub download meta-llama/CodeLlama-7b-Instruct --local-dir ./downloaded_model
   python3 convert.py --outtype q4_k_m --outfile models/codellama-7b.gguf downloaded_model/
   ```

3. **启动支持 OpenAI API 的服务器:**
   ```bash
   ./server -m models/codellama-7b.gguf --host 127.0.0.1 --port 8080 -c 2048 --embedding --parallel 2 --mlock -ngl 1
   ```

4. **配置 VSCode 插件:**
   - 针对 Continue 以及其他相关插件，配置其使用本地 API 服务器。

## 硬件优化

- **针对 Intel i9-11900KF:**  
  在编译时启用 AVX-512 指令集，并合理调整线程数（建议在 16 核中使用 12 至 14 核以获得最佳性能）。

- **针对 AMD RX 580:**  
  利用 ROCm 框架，仅将大约 30% 至 40% 的模型层分配给 GPU（例如，使用参数 `--n-gpu-layers 35`）。

## 参考资料

*在此处补充相关文档和资源链接。*