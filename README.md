# GC-Forged-Pylot

> **[Visit our landing page](https://nickscherbakov.github.io/GC-Forged-Pylot/)** for a visual introduction to the project.

**GC-Forged-Pylot** is an autonomous 24/7 coding system built on [llama.cpp](https://github.com/ggerganov/llama.cpp) with integrated GitHub Copilot functionality. It runs a local LLM server that provides an OpenAI-compatible API, supports chat and text completion endpoints, and allows for real-time streaming via WebSockets.

## Features

- **Local LLM Server:** Launch a local server using FastAPI that wraps a llama.cpp model.
- **API Endpoints:** Offers endpoints for status, completions, chat completions, and configuration.
- **Streaming Support:** Provides streaming responses for real-time data through HTTP and WebSocket protocols.
- **Caching:** Uses an in-memory cache to speed up repeated requests.
- **Environment Configurable:** Loads configuration from a JSON file and environment variables.
- **Hardware Optimization:** Automatically detects hardware capabilities and optimizes LLM parameters.
- **Self-Improvement:** Features a continuous learning loop that allows the system to refine itself.
- **Autonomous Operation:** Can run in autonomous mode, completing tasks with minimal human input.
- **üéÆ Gamification System:** Achievement system with levels, XP, and leaderboards to motivate contributors.
- **üîå Plugin Architecture:** Extensible plugin system for adding custom functionality without modifying core code.

## Advanced Capabilities

### Hardware Optimization System

GC-Forged-Pylot includes a sophisticated hardware optimization system that:

- **Auto-detects hardware** - Identifies CPU, RAM, and GPU specifications
- **Optimizes compilation** - Selects optimal flags for compiling llama.cpp
- **Tunes runtime parameters** - Determines optimal thread count, context size, batch size, and GPU layers
- **Benchmarks performance** - Tests various configurations to find the optimal setup
- **Persists profiles** - Saves hardware profiles for subsequent launches
- **Mock benchmarking** - Can run simulated benchmarks on systems without development tools

```bash
# Optimize hardware settings and parameters
python optimize_llama.py --benchmark --model ./models/your-model.gguf

# Skip compilation on systems without development tools
python optimize_llama.py --benchmark --skip-compilation --model ./models/your-model.gguf
```

### Self-Improvement System

The system includes a self-improvement mechanism that allows it to:

- **Analyze tasks** - Determine required capabilities for task completion
- **Generate code** - Create new modules to address missing functionality
- **Evaluate results** - Assess the quality of its own outputs
- **Learn from feedback** - Process user feedback to improve future performance
- **Autonomous cycles** - Run continuous improvement loops until reaching confidence threshold

```bash
# Execute a task with self-improvement cycles
python run_autonomous.py "Your task description" --cycles 5 --threshold 0.9

# Run in continuous self-improvement mode
python run_autonomous.py "Your task description" --continuous
```

## Project Structure

```
GC-Forged-Pylot/
‚îú‚îÄ‚îÄ main.py                # Entry point for the LLM server
‚îú‚îÄ‚îÄ optimize_llama.py      # Hardware optimization script
‚îú‚îÄ‚îÄ run_autonomous.py      # Autonomous agent runner with self-improvement
‚îú‚îÄ‚îÄ setup.py               # Package setup script and dependencies
‚îú‚îÄ‚îÄ config/                # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ agent_config.json  # Agent configuration
‚îÇ   ‚îú‚îÄ‚îÄ hardware_profile.json # Hardware optimization profile
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hardware_optimizer.py # Hardware detection and optimization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server.py      # Server implementation and API endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...            # Other core modules
‚îÇ   ‚îú‚îÄ‚îÄ self_improvement.py # Self-improvement and continuous learning module
‚îÇ   ‚îú‚îÄ‚îÄ bridge/            # Bridges to external systems and tools
‚îÇ   ‚îî‚îÄ‚îÄ pylot-agent/       # Autonomous agent implementation
‚îî‚îÄ‚îÄ tests/
    ‚îî‚îÄ‚îÄ core/              # Test modules
```

## Installation

1. **Clone the repository:**

   ```bash
   git clone https://github.com/yourusername/gc-forged-pylot.git
   cd gc-forged-pylot
   ```

2. **Create and activate a virtual environment:**

   ```bash
   python -m venv venv
   venv\Scripts\activate
   ```

3. **Install dependencies:**

   ```bash
   pip install -r requirements.txt  # or use setup.py by running: pip install -e .
   ```

4. **Set up environment variables:**

   Create a `.env` file in the project root with required environment variables (e.g. `GC_MODEL_PATH`).

## Running the Server

Run the server by executing the entry point:

```bash
python main.py --config config.json --host 127.0.0.1 --port 8000 --reload
```

The server uses [uvicorn](https://www.uvicorn.org/) to serve the FastAPI application. You should see logs indicating the model is being loaded and the server is running at the specified host and port.

## API Endpoints

The server provides several endpoints:

- `GET /v1/status`  
  Returns server status, including uptime, model info, active connections, and cache statistics.

- `GET /v1/models`  
  Lists available models.

- `POST /v1/completions`  
  Generates text completions for the given prompt. Supports optional streaming if requested.

- `POST /v1/chat/completions`  
  Processes chat completions based on an array of messages.

- **WebSocket Endpoint:**  
  `ws://<host>:<port>/ws/completions` for streaming completions and chat in real time.

- `GET /v1/config` and `POST /v1/config`  
  Retrieve and update the current server configuration.

For more details on request payloads and responses, check the API models defined in [server.py](e:\GC-Forged-Pylot\src\core\server.py).

## Development & Testing

- **Run Tests:**  
  Use your preferred test runner (e.g. pytest) to run tests located in the `tests/` folder.

  ```bash
  pytest
  ```

- **Auto-reload:**  
  Use the `--reload` flag when starting the server for automatic code reloading during development.

## üéÆ Community & Gamification

GC-Forged-Pylot features a unique gamification system to encourage and reward contributions!

### Achievement System
- Earn XP for contributions (commits, PRs, bug fixes, documentation, etc.)
- Unlock 12+ achievements
- Progress through 6 levels: –ù–æ–≤–∏—á–æ–∫ ‚Üí –ü—Ä–∞–∫—Ç–∏–∫–∞–Ω—Ç ‚Üí –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ ‚Üí –≠–∫—Å–ø–µ—Ä—Ç ‚Üí –ú–∞—Å—Ç–µ—Ä ‚Üí –õ–µ–≥–µ–Ω–¥–∞
- Compete on the leaderboard

See [CONTRIBUTING.md](CONTRIBUTING.md) for details on how to earn achievements!

### Plugin System
Extend GC-Forged-Pylot with custom plugins:
- Discord/Slack notifications
- Code review automation
- Performance monitoring
- Custom integrations

See [docs/PLUGIN_DEVELOPMENT.md](docs/PLUGIN_DEVELOPMENT.md) for plugin development guide.

### üèÜ Top Contributors
<!-- This section will be automatically updated -->
Coming soon - leaderboard integration!

## üöÄ Innovative Features

We're building more than just an AI assistant. Check out our vision:
- **20+ Innovative Ideas**: See [docs/INNOVATIVE_FEATURES.md](docs/INNOVATIVE_FEATURES.md)
- **AI Playground**: Interactive sandbox for experimenting with AI
- **Challenge System**: Weekly and monthly coding challenges
- **AI University**: Structured learning paths
- **Showcase Gallery**: Projects built with GC-Forged-Pylot
- **P2P Network**: Decentralized agent collaboration

## Contributing

Contributions are welcome! We have a gamified contribution system with achievements and rewards.

Please read [CONTRIBUTING.md](CONTRIBUTING.md) to learn:
- How to get started
- Coding standards
- How to earn achievements
- Available challenges
- Community guidelines

## License

This project is licensed under the [MIT License](LICENSE).

## Acknowledgments

- Inspired by [llama.cpp](https://github.com/ggerganov/llama.cpp) and GitHub Copilot.
- Built by the GC-Forged-Pylot Team.
